{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66acbe-a9e7-4334-919f-8713b1e93615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este arquivo cria o banco vetorial e persiste no disco.\n",
    "# Você poderá chamá-lo uma única vez sempre que adicionar/alterar PDFs.\n",
    "\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ferramentas para lidar com PDFs e dividir textos\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Para embeddings e banco vetorial\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def criar_base_vetorial():\n",
    "    \"\"\"\n",
    "    Função que:\n",
    "    1) Carrega PDFs de 'INs_Ancine_PDFs'\n",
    "    2) Separa em chunks\n",
    "    3) Gera embeddings e cria base Chroma persistida em 'db_vetorial'\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Carrega as variáveis de ambiente\n",
    "    load_dotenv()  # Lê o arquivo .env no diretório atual ou acima\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        print(\"Aviso: OPENAI_API_KEY não foi encontrada em .env\")\n",
    "\n",
    "    # 2. Carrega PDFs\n",
    "    pasta_pdfs = \"INs_Ancine_PDFs\"\n",
    "    pdf_files = [\n",
    "        os.path.join(pasta_pdfs, f)\n",
    "        for f in os.listdir(pasta_pdfs)\n",
    "        if f.lower().endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "    all_docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    print(f\"Total de PDFs encontrados: {len(pdf_files)}\")\n",
    "    print(f\"Total de documentos carregados: {len(all_docs)}\")\n",
    "\n",
    "    # 3. Divide em chunks\n",
    "    chunk_size = 1500\n",
    "    chunk_overlap = 300\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Priorizar quebras de parágrafo\n",
    "    )\n",
    "    \n",
    "    # Extrair e armazenar metadados ANTES do chunking\n",
    "    docs_with_metadata = []\n",
    "    for doc in all_docs:\n",
    "        # Extrair número da IN do título do documento ou conteúdo\n",
    "        in_match = re.search(r'IN\\s+n[º°\\.]\\s*(\\d+)', doc.page_content, re.IGNORECASE) or \\\n",
    "                  re.search(r'Instru[çc][aã]o\\s+Normativa\\s+n[º°\\.]\\s*(\\d+)', doc.page_content, re.IGNORECASE) or \\\n",
    "                  re.search(r'Instru[çc][aã]o\\s+Normativa\\s+(\\d+)', doc.page_content, re.IGNORECASE)\n",
    "        \n",
    "        if in_match:\n",
    "            doc.metadata['instrucao_normativa'] = f\"IN {in_match.group(1)}\"\n",
    "        \n",
    "        # Também pode usar o nome do arquivo como referência\n",
    "        if 'source' in doc.metadata:\n",
    "            filename = os.path.basename(doc.metadata['source'])\n",
    "            in_file_match = re.search(r'IN[-_\\s]*(\\d+)', filename, re.IGNORECASE)\n",
    "            if in_file_match and 'instrucao_normativa' not in doc.metadata:\n",
    "                doc.metadata['instrucao_normativa'] = f\"IN {in_file_match.group(1)}\"\n",
    "        \n",
    "        docs_with_metadata.append(doc)\n",
    "    \n",
    "    # Agora realize o chunking preservando os metadados\n",
    "    text_chunks = text_splitter.split_documents(docs_with_metadata)\n",
    "\n",
    "    print(f\"Total de chunks gerados: {len(text_chunks)}\")\n",
    "\n",
    "    # 4. Cria embeddings e base vetorial (Chroma)\n",
    "    embedding_engine = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'}  # ou 'cuda' se estiver configurado\n",
    "    )\n",
    "\n",
    "    db_pasta = \"db_vetorial\"  # Pasta de persistência do Chroma\n",
    "    vector_db = FAISS.from_documents(text_chunks, embedding_engine)\n",
    "    vector_db.save_local(\"db_vetorial\")  # persiste os arquivos, como index.faiss\n",
    "    # Em versões Chroma >= 0.4.x, persist() é automático\n",
    "    # vector_db.persist()  # Se precisar, dependendo da sua versão\n",
    "    print(\"Base vetorial criada.\")\n",
    "\n",
    "    # 5. Exibir tamanho em MB\n",
    "    # Chroma hoje costuma chamar o arquivo principal de \"chroma.sqlite3\" ou algo similar\n",
    "    # Então vou checar os arquivos .sqlite3 que encontrei na pasta\n",
    "    tamanho_total = 0.0\n",
    "    for f in os.listdir(db_pasta):\n",
    "        if f.endswith(\".sqlite\") or f.endswith(\".sqlite3\"):\n",
    "            full_path = os.path.join(db_pasta, f)\n",
    "            tamanho_total += os.path.getsize(full_path)\n",
    "\n",
    "    tamanho_mb = tamanho_total / (1024 * 1024)  # converte bytes -> MB\n",
    "    print(f\"Tamanho total da base: {tamanho_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    criar_base_vetorial()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cd222f-664e-4edd-80a7-ac3774a31b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ferramentas para lidar com PDFs e dividir textos\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Para embeddings e banco vetorial\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3160fc6f-d4eb-46bf-9542-3e4720e0e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Guilherme Gustavo Roca Arenales\n",
      "\n",
      "langchain_community     : 0.3.21\n",
      "langchain_text_splitters: 0.3.8\n",
      "langchain_huggingface   : 0.1.2\n",
      "re                      : 2.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Guilherme Gustavo Roca Arenales\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9e2cf-60a7-4d98-a832-976d76f0f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain e submódulos\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.schema import BaseRetriever\n",
    "\n",
    "def init_session_state():\n",
    "    \"\"\"Inicializa variáveis de estado da sessão do Streamlit\"\"\"\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    if \"memory\" not in st.session_state:\n",
    "        st.session_state.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\"\n",
    "        )\n",
    "    if \"chain\" not in st.session_state:\n",
    "        st.session_state.chain = None\n",
    "\n",
    "\n",
    "def extract_metadata_from_content(doc: Document) -> Document:\n",
    "    \"\"\"Extrai metadados do conteúdo do documento para enriquecê-lo\"\"\"\n",
    "    content = doc.page_content\n",
    "    \n",
    "    # Melhorar a extração de IN com regex mais abrangente\n",
    "    in_match = re.search(r'IN\\s+n[º°\\.]\\s*(\\d+)', content, re.IGNORECASE) or \\\n",
    "               re.search(r'Instru[çc][aã]o\\s+Normativa\\s+n[º°\\.]\\s*(\\d+)', content, re.IGNORECASE) or \\\n",
    "               re.search(r'Instru[çc][aã]o\\s+Normativa\\s+(\\d+)', content, re.IGNORECASE)\n",
    "    \n",
    "    # Extrair menções a artigos com regex mais amplo\n",
    "    art_matches = re.findall(r'Art[\\.\\s]*\\s*(\\d+)', content, re.IGNORECASE) or \\\n",
    "                  re.findall(r'Artigo\\s+(\\d+)', content, re.IGNORECASE)\n",
    "    \n",
    "    # Extrair incisos com numeração romana\n",
    "    inciso_matches = re.findall(r'(?:inciso|item)\\s+([IVXLCDM]+)', content, re.IGNORECASE)\n",
    "    \n",
    "    # Atualizar metadados\n",
    "    if in_match:\n",
    "        doc.metadata['instrucao_normativa'] = f\"IN {in_match.group(1)}\"\n",
    "    \n",
    "    if art_matches:\n",
    "        doc.metadata['artigos'] = [f\"Art. {art}\" for art in art_matches]\n",
    "        \n",
    "    if inciso_matches:\n",
    "        doc.metadata['incisos'] = [f\"inciso {inciso}\" for inciso in inciso_matches]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "def enrich_documents(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"Enriquece documentos com metadados extraídos e formatação\"\"\"\n",
    "    enriched_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        # Extrair metadados do conteúdo\n",
    "        doc = extract_metadata_from_content(doc)\n",
    "        \n",
    "        # Adicionar informações de fonte para facilitar citações\n",
    "        source_info = \"\"\n",
    "        if 'instrucao_normativa' in doc.metadata:\n",
    "            source_info += f\"{doc.metadata['instrucao_normativa']} - \"\n",
    "        if 'artigos' in doc.metadata and doc.metadata['artigos']:\n",
    "            source_info += f\"{', '.join(doc.metadata['artigos'])} - \"\n",
    "        if 'source' in doc.metadata:\n",
    "            filename = os.path.basename(doc.metadata['source'])\n",
    "            source_info += f\"Fonte: {filename}\"\n",
    "        \n",
    "        # Formatar conteúdo para melhor leitura pelo LLM\n",
    "        formatted_content = f\"\"\"\n",
    "TRECHO {i+1}:\n",
    "{source_info}\n",
    "---\n",
    "{doc.page_content}\n",
    "---\n",
    "\"\"\"\n",
    "        doc.page_content = formatted_content\n",
    "        enriched_docs.append(doc)\n",
    "    \n",
    "    return enriched_docs\n",
    "\n",
    "\n",
    "def create_retriever():\n",
    "    \"\"\"Cria o retriever melhorado para busca de documentos\"\"\"\n",
    "    try:\n",
    "        # Configuração do modelo de embeddings\n",
    "        embedding_engine = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        # Carrega a base vetorial\n",
    "        db_pasta = \"db_vetorial\"\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=db_pasta,\n",
    "            embedding_function=embedding_engine\n",
    "        )\n",
    "\n",
    "        # Função de filtragem personalizada\n",
    "        def filter_by_metadata_richness(docs):\n",
    "            \"\"\"Filtra documentos com base na riqueza de metadados\"\"\"\n",
    "            scored_docs = []\n",
    "            for doc in docs:\n",
    "                score = 0\n",
    "                if 'instrucao_normativa' in doc.metadata:\n",
    "                    score += 3  # Prioridade para documentos com IN\n",
    "                if 'artigos' in doc.metadata:\n",
    "                    score += len(doc.metadata['artigos'])\n",
    "                if 'source' in doc.metadata:\n",
    "                    score += 1  # Pontuação básica para documentos com fonte\n",
    "                \n",
    "                scored_docs.append((doc, score))\n",
    "            \n",
    "            # Seleciona os melhores documentos\n",
    "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "            return [doc for doc, _ in scored_docs[:10]]  # Top 10 documentos\n",
    "\n",
    "        # Configuração do retriever base com MMR\n",
    "        base_retriever = vector_db.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": 10,\n",
    "                \"fetch_k\": 30\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Implementação do Custom Retriever\n",
    "        class CustomRetriever(BaseRetriever):\n",
    "            \"\"\"Retriever personalizado que combina MMR com filtragem pós-processamento\"\"\"\n",
    "            \n",
    "            def _get_relevant_documents(self, query: str, *, run_manager) -> List[Document]:\n",
    "                # Primeiro obtém os documentos do retriever base\n",
    "                base_docs = base_retriever.get_relevant_documents(query)\n",
    "                # Depois aplica o filtro personalizado\n",
    "                return filter_by_metadata_richness(base_docs)\n",
    "\n",
    "            async def _aget_relevant_documents(self, query: str, *, run_manager):\n",
    "                # Implementação assíncrona se necessário\n",
    "                return self._get_relevant_documents(query)\n",
    "\n",
    "        # Cria e retorna o retriever personalizado\n",
    "        return CustomRetriever()\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Erro ao criar o retriever: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_chain():\n",
    "    \"\"\"Cria a chain RAG com configurações personalizadas\"\"\"\n",
    "    try:\n",
    "        # Carrega variáveis de ambiente\n",
    "        load_dotenv()\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        if not openai_api_key:\n",
    "            st.error(\"API Key da OpenAI não encontrada. Configure-a no arquivo .env\")\n",
    "            return None\n",
    "        \n",
    "        # Cria o retriever\n",
    "        retriever = create_retriever()\n",
    "        if not retriever:\n",
    "            return None\n",
    "        \n",
    "        # Define o prompt personalizado com instruções mais flexíveis para o LLM\n",
    "        system_template = \"\"\"\\\n",
    "Você é um assistente especializado em legislação da ANCINE (Agência Nacional do Cinema), responsável por fornecer informações precisas sobre instruções normativas.\n",
    "\n",
    "--- Conteúdo relevante da base de dados sobre INs da ANCINE ---\n",
    "{context}\n",
    "\n",
    "---- INSTRUÇÕES CRÍTICAS ----\n",
    "VOCÊ DEVE SEMPRE CITAR EXPLICITAMENTE OS NÚMEROS DAS INs QUANDO DISPONÍVEIS. Esta é uma EXIGÊNCIA ABSOLUTA.\n",
    "\n",
    "1) SEMPRE comece sua resposta identificando em qual IN a informação foi encontrada. Use exatamente a numeração que aparece nos trechos.\n",
    "   Exemplo correto: \"De acordo com a IN nº 95, ...\"\n",
    "   Exemplo incorreto: \"Em uma das INs da ANCINE, consta que...\"\n",
    "\n",
    "2) Se você identificar o conteúdo mas não encontrar o número da IN nos trechos fornecidos, PROCURE NOS METADADOS:\n",
    "   - Procure por tags como [instrucao_normativa: IN XX] ou frases que indiquem o documento de origem\n",
    "   - Verifique o nome do arquivo na referência (geralmente contém o número da IN)\n",
    "\n",
    "3) Se você ainda não conseguir determinar o número da IN, então e SOMENTE então, use \"Segundo regulamentação da ANCINE\" e continue com informações precisas.\n",
    "\n",
    "4) SEMPRE inclua citações específicas a artigos, parágrafos e incisos quando presentes.\n",
    "\n",
    "5) NUNCA responda de forma genérica quando informações específicas estiverem disponíveis.\n",
    "\n",
    "Este sistema é uma ferramenta de consulta legal e DEVE fornecer referências precisas. Respostas sem citações específicas são consideradas falhas.\n",
    "\n",
    "# Adicionar ao seu prompt:\n",
    "\n",
    "Técnicas que você DEVE usar para responder corretamente:\n",
    "\n",
    "1) EXAMINE TODOS OS TRECHOS FORNECIDOS em busca de menções a números de INs. Não se limite aos primeiros trechos.\n",
    "\n",
    "2) EXAMINE OS METADADOS de cada documento. Procure por padrões como:\n",
    "   - [instrucao_normativa: IN XX]\n",
    "   - Nomes de arquivos como \"IN_95_texto.pdf\" \n",
    "   - Referências explícitas no texto como \"Instrução Normativa nº XX\"\n",
    "\n",
    "3) Se o mesmo conteúdo aparecer em múltiplos trechos, VERIFIQUE se algum deles contém o número da IN.\n",
    "\n",
    "4) Quando mencionar artigos ou incisos, SEMPRE indique a qual IN pertence.\n",
    "\n",
    "5) Prefira ser específico mesmo que só tenha certeza parcial. É melhor dizer \"A informação parece constar na IN 95, conforme indicado no texto\" do que omitir a referência.\n",
    "\n",
    "EXEMPLOS DE RESPOSTAS INACEITÁVEIS:\n",
    "- \"Em uma das INs da ANCINE, consta que...\"\n",
    "- \"Segundo as instruções normativas da ANCINE...\"\n",
    "- \"Para entender as diferenças... é importante considerar as definições e requisitos estabelecidos nas instruções normativas da ANCINE.\"\n",
    "\n",
    "EXEMPLO DE RESPOSTA ACEITÁVEL:\n",
    "\"De acordo com a IN nº 95, Art. 2º, inciso VII, uma obra audiovisual brasileira é definida como...\"\n",
    "\n",
    "\n",
    "\n",
    "Agora, responda à pergunta do usuário com todas as referências específicas que conseguir extrair dos trechos:\n",
    "{question}\n",
    "\"\"\"\n",
    "        \n",
    "        prompt_template = PromptTemplate(\n",
    "            template=system_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Configura o modelo LLM\n",
    "        llm = ChatOpenAI(\n",
    "            openai_api_key=openai_api_key,\n",
    "            model=\"o3-mini\",\n",
    "            #temperature=0.1,\n",
    "            max_tokens=6000,\n",
    "            request_timeout=90,\n",
    "        )\n",
    "\n",
    "        # Cria a chain com memória e prompt customizado\n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            memory=st.session_state.memory,\n",
    "            chain_type=\"stuff\",\n",
    "            combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
    "            return_source_documents=True,\n",
    "            output_key=\"answer\",\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "        # Função para processar a query antes de enviar ao retriever\n",
    "        def process_query(query):\n",
    "            # Expandir a query para melhorar a recuperação\n",
    "            expansions = {\n",
    "                \"cadastrar\": [\"cadastro\", \"registro\", \"registrar\", \"inscrição\", \"inscrever\"],\n",
    "                \"empresa\": [\"empresa\", \"produtora\", \"distribuidor\", \"distribuidora\", \"companhia\", \"pessoa jurídica\"],\n",
    "                \"publicidade\": [\"publicidade\", \"propaganda\", \"anúncio\", \"comercial\", \"intervalo comercial\"],\n",
    "                \"limite\": [\"limite\", \"limitação\", \"tempo\", \"duração\", \"restrição\"],\n",
    "                \"SCB\": [\"Sistema de Controle de Bilheteria\"],\n",
    "                \"SADIS\": [\"Sistema de Acompanhamento de Distribuição em Salas\"],\n",
    "                \"SAVI\": [\"Sistema de Acompanhamento de Video Doméstico\"],\n",
    "                \"SRPTV\": [\"Sistema de Recepção de Programação de TV Paga\"],\n",
    "                \"SAD\": [\"Sistema Ancine Digital\"],\n",
    "                \"CPB\": [\"Certificado de Produto Brasileiro\"],\n",
    "                \"ROE\": [\"Registro de Obra Estrangeira\"],\n",
    "                \"CRT\": [\"Certificado de Registro de Título\"]\n",
    "            }\n",
    "            \n",
    "            expanded_query = query\n",
    "            for key, alternatives in expansions.items():\n",
    "                if key.lower() in query.lower():\n",
    "                    for alt in alternatives:\n",
    "                        if alt.lower() not in query.lower():\n",
    "                            expanded_query += f\" {alt}\"\n",
    "            \n",
    "            return expanded_query\n",
    "        \n",
    "        # Criamos um wrapper para a chain original que processará os documentos\n",
    "        def chain_with_preprocessing(inputs):\n",
    "            # Processar a query para melhorar a recuperação\n",
    "            question = inputs[\"question\"]\n",
    "            expanded_question = process_query(question)\n",
    "            \n",
    "            # NOVO CÓDIGO ADICIONADO AQUI\n",
    "            # Recuperar documentos relevantes\n",
    "            docs = retriever.get_relevant_documents(expanded_question)\n",
    "            \n",
    "            # Fazer análise preliminar das INs e artigos\n",
    "            ins_mentioned = {}\n",
    "            for doc in docs:\n",
    "                if 'instrucao_normativa' in doc.metadata:\n",
    "                    in_num = doc.metadata['instrucao_normativa']\n",
    "                    if in_num not in ins_mentioned:\n",
    "                        ins_mentioned[in_num] = []\n",
    "                        \n",
    "                    # Extrair artigos mencionados no conteúdo\n",
    "                    art_pattern = r'Art[\\.\\s]*\\s*(\\d+)'\n",
    "                    art_matches = re.findall(art_pattern, doc.page_content, re.IGNORECASE)\n",
    "                    ins_mentioned[in_num].extend([f\"Art. {m}\" for m in art_matches])\n",
    "            \n",
    "            # Criar resumo contextual\n",
    "            context_summary = \"\\n\\nINFORMAÇÕES DE CONTEXTO IMPORTANTES:\\n\"\n",
    "            for in_num, articles in ins_mentioned.items():\n",
    "                unique_articles = list(set(articles))\n",
    "                context_summary += f\"- {in_num} contém os seguintes artigos relevantes: {', '.join(unique_articles[:10])}\\n\"\n",
    "            \n",
    "            # Combinar com a query expandida\n",
    "            final_question = f\"{expanded_question}\\n\\n{context_summary}\"\n",
    "            # FIM DO NOVO CÓDIGO\n",
    "            \n",
    "            # Executar a chain com a query final aprimorada\n",
    "            result = chain({\"question\": final_question})\n",
    "            \n",
    "            # Processar os documentos de origem antes de retornar\n",
    "            if \"source_documents\" in result:\n",
    "                result[\"source_documents\"] = enrich_documents(result[\"source_documents\"])\n",
    "                \n",
    "            return result\n",
    "        \n",
    "        # Retornamos a função wrapped em vez da chain direta\n",
    "        return chain_with_preprocessing\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Erro ao criar a chain: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_chat_history():\n",
    "    \"\"\"Exibe o histórico de conversa na interface\"\"\"\n",
    "    for message in st.session_state.messages:\n",
    "        if message[\"role\"] == \"human\":\n",
    "            with st.chat_message(\"user\"):\n",
    "                st.write(message[\"content\"])\n",
    "        else:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(message[\"content\"])\n",
    "\n",
    "\n",
    "def extract_reference_summary(docs):\n",
    "    \"\"\"Extrai um resumo das referências encontradas para mostrar ao usuário\"\"\"\n",
    "    if not docs:\n",
    "        return \"Nenhuma fonte específica encontrada.\"\n",
    "    \n",
    "    references = []\n",
    "    ins_mentioned = set()\n",
    "    articles_mentioned = set()\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Extrai IN\n",
    "        if 'instrucao_normativa' in doc.metadata:\n",
    "            ins_mentioned.add(doc.metadata['instrucao_normativa'])\n",
    "        \n",
    "        # Extrai artigos mencionados\n",
    "        if 'artigos' in doc.metadata and doc.metadata['artigos']:\n",
    "            for art in doc.metadata['artigos']:\n",
    "                articles_mentioned.add(art)\n",
    "        \n",
    "        # Extrai fonte do documento\n",
    "        if 'source' in doc.metadata:\n",
    "            filename = os.path.basename(doc.metadata['source'])\n",
    "            references.append(filename)\n",
    "    \n",
    "    # Formata resumo\n",
    "    summary = []\n",
    "    if articles_mentioned:\n",
    "        summary.append(f\"**Artigos mencionados:** {', '.join(sorted(articles_mentioned))}\")\n",
    "    \n",
    "    if ins_mentioned:\n",
    "        summary.append(f\"**Instruções Normativas:** {', '.join(sorted(ins_mentioned))}\")\n",
    "    \n",
    "    unique_refs = list(set(references))\n",
    "    if unique_refs:\n",
    "        summary.append(f\"**Documentos consultados:** {', '.join(unique_refs[:3])}\")\n",
    "    \n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuração da página\n",
    "    st.set_page_config(\n",
    "        page_title=\"Chatbot da ANCINE\",\n",
    "        page_icon=\"🎬\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    # Inicializa variáveis de estado\n",
    "    init_session_state()\n",
    "    \n",
    "    # Interface principal\n",
    "    st.title(\"🎬 Chatbot de Instruções Normativas da ANCINE\")\n",
    "    st.markdown(\"\"\"\n",
    "    Tire suas dúvidas sobre instruções normativas e regulamentações do segmento de regulação e registro do mercado audiovisual brasileiro .\n",
    "    Este chatbot consulta a base de documentos oficiais da ANCINE para fornecer informações precisas.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Sidebar para configurações\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configurações\")\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            if st.button(\"🔄 Reiniciar Chat\", use_container_width=True):\n",
    "                st.session_state.chain = None\n",
    "                st.success(\"Chatbot reiniciado.\")\n",
    "        \n",
    "        with col2:\n",
    "            if st.button(\"🗑️ Limpar Histórico\", use_container_width=True):\n",
    "                st.session_state.messages = []\n",
    "                st.session_state.memory = ConversationBufferMemory(\n",
    "                    memory_key=\"chat_history\",\n",
    "                    return_messages=True,\n",
    "                    output_key=\"answer\"\n",
    "                )\n",
    "                st.session_state.chain = None\n",
    "                st.rerun()  # Substituído experimental_rerun por rerun\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"\"\"\n",
    "        ### Exemplos de perguntas:\n",
    "        - O que é uma obra brasileira independente?\n",
    "        - Quais os requisitos para cadastro de obra?\n",
    "        - Como faço para registrar um projeto na ANCINE?\n",
    "        - O que diz a IN 95 sobre produção?\n",
    "        - Classificação indicativa segundo a ANCINE\n",
    "        \"\"\")\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"Desenvolvido com ❤️ usando LangChain e Streamlit para a ENAP\")\n",
    "    \n",
    "    # Inicializa a chain se ainda não existir\n",
    "    if st.session_state.chain is None:\n",
    "        with st.spinner(\"Inicializando o chatbot especialista em ANCINE...\"):\n",
    "            st.session_state.chain = create_chain()\n",
    "            if st.session_state.chain is None:\n",
    "                st.error(\"Não foi possível inicializar o chatbot. Verifique sua API Key no arquivo .env\")\n",
    "                return\n",
    "    \n",
    "    # Exibe histórico de mensagens\n",
    "    display_chat_history()\n",
    "    \n",
    "    # Campo de entrada do usuário\n",
    "    user_query = st.chat_input(\"Digite sua pergunta sobre instruções normativas da ANCINE...\")\n",
    "    \n",
    "    # Processamento da pergunta\n",
    "    if user_query:\n",
    "        # Adiciona a pergunta ao histórico\n",
    "        st.session_state.messages.append({\"role\": \"human\", \"content\": user_query})\n",
    "        \n",
    "        # Exibe a pergunta\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(user_query)\n",
    "        \n",
    "        # Exibe indicador de \"pensando\"\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            message_placeholder.markdown(\"🔍 Consultando a base de INs da ANCINE...\")\n",
    "            \n",
    "            try:\n",
    "                # Executa a chain para obter a resposta\n",
    "                start_time = time.time()\n",
    "                response = st.session_state.chain({\"question\": user_query})\n",
    "                end_time = time.time()\n",
    "                \n",
    "                # Formata a resposta\n",
    "                answer = response.get(\"answer\", \"Desculpe, não consegui encontrar informações sobre este tema nas INs consultadas.\")\n",
    "                \n",
    "                # Análise e resumo das fontes\n",
    "                source_docs = response.get(\"source_documents\", [])\n",
    "                if source_docs:\n",
    "                    reference_summary = extract_reference_summary(source_docs)\n",
    "                    answer += f\"\\n\\n---\\n{reference_summary}\"\n",
    "                \n",
    "                # Adiciona tempo de resposta\n",
    "                answer += f\"\\n\\n*Tempo de resposta: {end_time - start_time:.2f} segundos*\"\n",
    "                \n",
    "                # Atualiza o placeholder com a resposta\n",
    "                message_placeholder.markdown(answer)\n",
    "                \n",
    "                # Salva a resposta no histórico\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Erro ao processar sua pergunta: {str(e)}\"\n",
    "                message_placeholder.error(error_msg)\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
