{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66acbe-a9e7-4334-919f-8713b1e93615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este arquivo cria o banco vetorial e persiste no disco.\n",
    "# Voc√™ poder√° cham√°-lo uma √∫nica vez sempre que adicionar/alterar PDFs.\n",
    "\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ferramentas para lidar com PDFs e dividir textos\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Para embeddings e banco vetorial\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def criar_base_vetorial():\n",
    "    \"\"\"\n",
    "    Fun√ß√£o que:\n",
    "    1) Carrega PDFs de 'INs_Ancine_PDFs'\n",
    "    2) Separa em chunks\n",
    "    3) Gera embeddings e cria base Chroma persistida em 'db_vetorial'\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Carrega as vari√°veis de ambiente\n",
    "    load_dotenv()  # L√™ o arquivo .env no diret√≥rio atual ou acima\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        print(\"Aviso: OPENAI_API_KEY n√£o foi encontrada em .env\")\n",
    "\n",
    "    # 2. Carrega PDFs\n",
    "    pasta_pdfs = \"INs_Ancine_PDFs\"\n",
    "    pdf_files = [\n",
    "        os.path.join(pasta_pdfs, f)\n",
    "        for f in os.listdir(pasta_pdfs)\n",
    "        if f.lower().endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "    all_docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    print(f\"Total de PDFs encontrados: {len(pdf_files)}\")\n",
    "    print(f\"Total de documentos carregados: {len(all_docs)}\")\n",
    "\n",
    "    # 3. Divide em chunks\n",
    "    chunk_size = 1500\n",
    "    chunk_overlap = 300\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Priorizar quebras de par√°grafo\n",
    "    )\n",
    "    \n",
    "    # Extrair e armazenar metadados ANTES do chunking\n",
    "    docs_with_metadata = []\n",
    "    for doc in all_docs:\n",
    "        # Extrair n√∫mero da IN do t√≠tulo do documento ou conte√∫do\n",
    "        in_match = re.search(r'IN\\s+n[¬∫¬∞\\.]\\s*(\\d+)', doc.page_content, re.IGNORECASE) or \\\n",
    "                  re.search(r'Instru[√ßc][a√£]o\\s+Normativa\\s+n[¬∫¬∞\\.]\\s*(\\d+)', doc.page_content, re.IGNORECASE) or \\\n",
    "                  re.search(r'Instru[√ßc][a√£]o\\s+Normativa\\s+(\\d+)', doc.page_content, re.IGNORECASE)\n",
    "        \n",
    "        if in_match:\n",
    "            doc.metadata['instrucao_normativa'] = f\"IN {in_match.group(1)}\"\n",
    "        \n",
    "        # Tamb√©m pode usar o nome do arquivo como refer√™ncia\n",
    "        if 'source' in doc.metadata:\n",
    "            filename = os.path.basename(doc.metadata['source'])\n",
    "            in_file_match = re.search(r'IN[-_\\s]*(\\d+)', filename, re.IGNORECASE)\n",
    "            if in_file_match and 'instrucao_normativa' not in doc.metadata:\n",
    "                doc.metadata['instrucao_normativa'] = f\"IN {in_file_match.group(1)}\"\n",
    "        \n",
    "        docs_with_metadata.append(doc)\n",
    "    \n",
    "    # Agora realize o chunking preservando os metadados\n",
    "    text_chunks = text_splitter.split_documents(docs_with_metadata)\n",
    "\n",
    "    print(f\"Total de chunks gerados: {len(text_chunks)}\")\n",
    "\n",
    "    # 4. Cria embeddings e base vetorial (Chroma)\n",
    "    embedding_engine = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'}  # ou 'cuda' se estiver configurado\n",
    "    )\n",
    "\n",
    "    db_pasta = \"db_vetorial\"  # Pasta de persist√™ncia do Chroma\n",
    "    vector_db = FAISS.from_documents(text_chunks, embedding_engine)\n",
    "    vector_db.save_local(\"db_vetorial\")  # persiste os arquivos, como index.faiss\n",
    "    # Em vers√µes Chroma >= 0.4.x, persist() √© autom√°tico\n",
    "    # vector_db.persist()  # Se precisar, dependendo da sua vers√£o\n",
    "    print(\"Base vetorial criada.\")\n",
    "\n",
    "    # 5. Exibir tamanho em MB\n",
    "    # Chroma hoje costuma chamar o arquivo principal de \"chroma.sqlite3\" ou algo similar\n",
    "    # Ent√£o vou checar os arquivos .sqlite3 que encontrei na pasta\n",
    "    tamanho_total = 0.0\n",
    "    for f in os.listdir(db_pasta):\n",
    "        if f.endswith(\".sqlite\") or f.endswith(\".sqlite3\"):\n",
    "            full_path = os.path.join(db_pasta, f)\n",
    "            tamanho_total += os.path.getsize(full_path)\n",
    "\n",
    "    tamanho_mb = tamanho_total / (1024 * 1024)  # converte bytes -> MB\n",
    "    print(f\"Tamanho total da base: {tamanho_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    criar_base_vetorial()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cd222f-664e-4edd-80a7-ac3774a31b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ferramentas para lidar com PDFs e dividir textos\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Para embeddings e banco vetorial\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3160fc6f-d4eb-46bf-9542-3e4720e0e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Guilherme Gustavo Roca Arenales\n",
      "\n",
      "langchain_community     : 0.3.21\n",
      "langchain_text_splitters: 0.3.8\n",
      "langchain_huggingface   : 0.1.2\n",
      "re                      : 2.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Guilherme Gustavo Roca Arenales\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9e2cf-60a7-4d98-a832-976d76f0f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain e subm√≥dulos\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.schema import BaseRetriever\n",
    "\n",
    "def init_session_state():\n",
    "    \"\"\"Inicializa vari√°veis de estado da sess√£o do Streamlit\"\"\"\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    if \"memory\" not in st.session_state:\n",
    "        st.session_state.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\"\n",
    "        )\n",
    "    if \"chain\" not in st.session_state:\n",
    "        st.session_state.chain = None\n",
    "\n",
    "\n",
    "def extract_metadata_from_content(doc: Document) -> Document:\n",
    "    \"\"\"Extrai metadados do conte√∫do do documento para enriquec√™-lo\"\"\"\n",
    "    content = doc.page_content\n",
    "    \n",
    "    # Melhorar a extra√ß√£o de IN com regex mais abrangente\n",
    "    in_match = re.search(r'IN\\s+n[¬∫¬∞\\.]\\s*(\\d+)', content, re.IGNORECASE) or \\\n",
    "               re.search(r'Instru[√ßc][a√£]o\\s+Normativa\\s+n[¬∫¬∞\\.]\\s*(\\d+)', content, re.IGNORECASE) or \\\n",
    "               re.search(r'Instru[√ßc][a√£]o\\s+Normativa\\s+(\\d+)', content, re.IGNORECASE)\n",
    "    \n",
    "    # Extrair men√ß√µes a artigos com regex mais amplo\n",
    "    art_matches = re.findall(r'Art[\\.\\s]*\\s*(\\d+)', content, re.IGNORECASE) or \\\n",
    "                  re.findall(r'Artigo\\s+(\\d+)', content, re.IGNORECASE)\n",
    "    \n",
    "    # Extrair incisos com numera√ß√£o romana\n",
    "    inciso_matches = re.findall(r'(?:inciso|item)\\s+([IVXLCDM]+)', content, re.IGNORECASE)\n",
    "    \n",
    "    # Atualizar metadados\n",
    "    if in_match:\n",
    "        doc.metadata['instrucao_normativa'] = f\"IN {in_match.group(1)}\"\n",
    "    \n",
    "    if art_matches:\n",
    "        doc.metadata['artigos'] = [f\"Art. {art}\" for art in art_matches]\n",
    "        \n",
    "    if inciso_matches:\n",
    "        doc.metadata['incisos'] = [f\"inciso {inciso}\" for inciso in inciso_matches]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "def enrich_documents(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"Enriquece documentos com metadados extra√≠dos e formata√ß√£o\"\"\"\n",
    "    enriched_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        # Extrair metadados do conte√∫do\n",
    "        doc = extract_metadata_from_content(doc)\n",
    "        \n",
    "        # Adicionar informa√ß√µes de fonte para facilitar cita√ß√µes\n",
    "        source_info = \"\"\n",
    "        if 'instrucao_normativa' in doc.metadata:\n",
    "            source_info += f\"{doc.metadata['instrucao_normativa']} - \"\n",
    "        if 'artigos' in doc.metadata and doc.metadata['artigos']:\n",
    "            source_info += f\"{', '.join(doc.metadata['artigos'])} - \"\n",
    "        if 'source' in doc.metadata:\n",
    "            filename = os.path.basename(doc.metadata['source'])\n",
    "            source_info += f\"Fonte: {filename}\"\n",
    "        \n",
    "        # Formatar conte√∫do para melhor leitura pelo LLM\n",
    "        formatted_content = f\"\"\"\n",
    "TRECHO {i+1}:\n",
    "{source_info}\n",
    "---\n",
    "{doc.page_content}\n",
    "---\n",
    "\"\"\"\n",
    "        doc.page_content = formatted_content\n",
    "        enriched_docs.append(doc)\n",
    "    \n",
    "    return enriched_docs\n",
    "\n",
    "\n",
    "def create_retriever():\n",
    "    \"\"\"Cria o retriever melhorado para busca de documentos\"\"\"\n",
    "    try:\n",
    "        # Configura√ß√£o do modelo de embeddings\n",
    "        embedding_engine = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        # Carrega a base vetorial\n",
    "        db_pasta = \"db_vetorial\"\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=db_pasta,\n",
    "            embedding_function=embedding_engine\n",
    "        )\n",
    "\n",
    "        # Fun√ß√£o de filtragem personalizada\n",
    "        def filter_by_metadata_richness(docs):\n",
    "            \"\"\"Filtra documentos com base na riqueza de metadados\"\"\"\n",
    "            scored_docs = []\n",
    "            for doc in docs:\n",
    "                score = 0\n",
    "                if 'instrucao_normativa' in doc.metadata:\n",
    "                    score += 3  # Prioridade para documentos com IN\n",
    "                if 'artigos' in doc.metadata:\n",
    "                    score += len(doc.metadata['artigos'])\n",
    "                if 'source' in doc.metadata:\n",
    "                    score += 1  # Pontua√ß√£o b√°sica para documentos com fonte\n",
    "                \n",
    "                scored_docs.append((doc, score))\n",
    "            \n",
    "            # Seleciona os melhores documentos\n",
    "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "            return [doc for doc, _ in scored_docs[:10]]  # Top 10 documentos\n",
    "\n",
    "        # Configura√ß√£o do retriever base com MMR\n",
    "        base_retriever = vector_db.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": 10,\n",
    "                \"fetch_k\": 30\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Implementa√ß√£o do Custom Retriever\n",
    "        class CustomRetriever(BaseRetriever):\n",
    "            \"\"\"Retriever personalizado que combina MMR com filtragem p√≥s-processamento\"\"\"\n",
    "            \n",
    "            def _get_relevant_documents(self, query: str, *, run_manager) -> List[Document]:\n",
    "                # Primeiro obt√©m os documentos do retriever base\n",
    "                base_docs = base_retriever.get_relevant_documents(query)\n",
    "                # Depois aplica o filtro personalizado\n",
    "                return filter_by_metadata_richness(base_docs)\n",
    "\n",
    "            async def _aget_relevant_documents(self, query: str, *, run_manager):\n",
    "                # Implementa√ß√£o ass√≠ncrona se necess√°rio\n",
    "                return self._get_relevant_documents(query)\n",
    "\n",
    "        # Cria e retorna o retriever personalizado\n",
    "        return CustomRetriever()\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Erro ao criar o retriever: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_chain():\n",
    "    \"\"\"Cria a chain RAG com configura√ß√µes personalizadas\"\"\"\n",
    "    try:\n",
    "        # Carrega vari√°veis de ambiente\n",
    "        load_dotenv()\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        if not openai_api_key:\n",
    "            st.error(\"API Key da OpenAI n√£o encontrada. Configure-a no arquivo .env\")\n",
    "            return None\n",
    "        \n",
    "        # Cria o retriever\n",
    "        retriever = create_retriever()\n",
    "        if not retriever:\n",
    "            return None\n",
    "        \n",
    "        # Define o prompt personalizado com instru√ß√µes mais flex√≠veis para o LLM\n",
    "        system_template = \"\"\"\\\n",
    "Voc√™ √© um assistente especializado em legisla√ß√£o da ANCINE (Ag√™ncia Nacional do Cinema), respons√°vel por fornecer informa√ß√µes precisas sobre instru√ß√µes normativas.\n",
    "\n",
    "--- Conte√∫do relevante da base de dados sobre INs da ANCINE ---\n",
    "{context}\n",
    "\n",
    "---- INSTRU√á√ïES CR√çTICAS ----\n",
    "VOC√ä DEVE SEMPRE CITAR EXPLICITAMENTE OS N√öMEROS DAS INs QUANDO DISPON√çVEIS. Esta √© uma EXIG√äNCIA ABSOLUTA.\n",
    "\n",
    "1) SEMPRE comece sua resposta identificando em qual IN a informa√ß√£o foi encontrada. Use exatamente a numera√ß√£o que aparece nos trechos.\n",
    "   Exemplo correto: \"De acordo com a IN n¬∫ 95, ...\"\n",
    "   Exemplo incorreto: \"Em uma das INs da ANCINE, consta que...\"\n",
    "\n",
    "2) Se voc√™ identificar o conte√∫do mas n√£o encontrar o n√∫mero da IN nos trechos fornecidos, PROCURE NOS METADADOS:\n",
    "   - Procure por tags como [instrucao_normativa: IN XX] ou frases que indiquem o documento de origem\n",
    "   - Verifique o nome do arquivo na refer√™ncia (geralmente cont√©m o n√∫mero da IN)\n",
    "\n",
    "3) Se voc√™ ainda n√£o conseguir determinar o n√∫mero da IN, ent√£o e SOMENTE ent√£o, use \"Segundo regulamenta√ß√£o da ANCINE\" e continue com informa√ß√µes precisas.\n",
    "\n",
    "4) SEMPRE inclua cita√ß√µes espec√≠ficas a artigos, par√°grafos e incisos quando presentes.\n",
    "\n",
    "5) NUNCA responda de forma gen√©rica quando informa√ß√µes espec√≠ficas estiverem dispon√≠veis.\n",
    "\n",
    "Este sistema √© uma ferramenta de consulta legal e DEVE fornecer refer√™ncias precisas. Respostas sem cita√ß√µes espec√≠ficas s√£o consideradas falhas.\n",
    "\n",
    "# Adicionar ao seu prompt:\n",
    "\n",
    "T√©cnicas que voc√™ DEVE usar para responder corretamente:\n",
    "\n",
    "1) EXAMINE TODOS OS TRECHOS FORNECIDOS em busca de men√ß√µes a n√∫meros de INs. N√£o se limite aos primeiros trechos.\n",
    "\n",
    "2) EXAMINE OS METADADOS de cada documento. Procure por padr√µes como:\n",
    "   - [instrucao_normativa: IN XX]\n",
    "   - Nomes de arquivos como \"IN_95_texto.pdf\" \n",
    "   - Refer√™ncias expl√≠citas no texto como \"Instru√ß√£o Normativa n¬∫ XX\"\n",
    "\n",
    "3) Se o mesmo conte√∫do aparecer em m√∫ltiplos trechos, VERIFIQUE se algum deles cont√©m o n√∫mero da IN.\n",
    "\n",
    "4) Quando mencionar artigos ou incisos, SEMPRE indique a qual IN pertence.\n",
    "\n",
    "5) Prefira ser espec√≠fico mesmo que s√≥ tenha certeza parcial. √â melhor dizer \"A informa√ß√£o parece constar na IN 95, conforme indicado no texto\" do que omitir a refer√™ncia.\n",
    "\n",
    "EXEMPLOS DE RESPOSTAS INACEIT√ÅVEIS:\n",
    "- \"Em uma das INs da ANCINE, consta que...\"\n",
    "- \"Segundo as instru√ß√µes normativas da ANCINE...\"\n",
    "- \"Para entender as diferen√ßas... √© importante considerar as defini√ß√µes e requisitos estabelecidos nas instru√ß√µes normativas da ANCINE.\"\n",
    "\n",
    "EXEMPLO DE RESPOSTA ACEIT√ÅVEL:\n",
    "\"De acordo com a IN n¬∫ 95, Art. 2¬∫, inciso VII, uma obra audiovisual brasileira √© definida como...\"\n",
    "\n",
    "\n",
    "\n",
    "Agora, responda √† pergunta do usu√°rio com todas as refer√™ncias espec√≠ficas que conseguir extrair dos trechos:\n",
    "{question}\n",
    "\"\"\"\n",
    "        \n",
    "        prompt_template = PromptTemplate(\n",
    "            template=system_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Configura o modelo LLM\n",
    "        llm = ChatOpenAI(\n",
    "            openai_api_key=openai_api_key,\n",
    "            model=\"o3-mini\",\n",
    "            #temperature=0.1,\n",
    "            max_tokens=6000,\n",
    "            request_timeout=90,\n",
    "        )\n",
    "\n",
    "        # Cria a chain com mem√≥ria e prompt customizado\n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            memory=st.session_state.memory,\n",
    "            chain_type=\"stuff\",\n",
    "            combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
    "            return_source_documents=True,\n",
    "            output_key=\"answer\",\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "        # Fun√ß√£o para processar a query antes de enviar ao retriever\n",
    "        def process_query(query):\n",
    "            # Expandir a query para melhorar a recupera√ß√£o\n",
    "            expansions = {\n",
    "                \"cadastrar\": [\"cadastro\", \"registro\", \"registrar\", \"inscri√ß√£o\", \"inscrever\"],\n",
    "                \"empresa\": [\"empresa\", \"produtora\", \"distribuidor\", \"distribuidora\", \"companhia\", \"pessoa jur√≠dica\"],\n",
    "                \"publicidade\": [\"publicidade\", \"propaganda\", \"an√∫ncio\", \"comercial\", \"intervalo comercial\"],\n",
    "                \"limite\": [\"limite\", \"limita√ß√£o\", \"tempo\", \"dura√ß√£o\", \"restri√ß√£o\"],\n",
    "                \"SCB\": [\"Sistema de Controle de Bilheteria\"],\n",
    "                \"SADIS\": [\"Sistema de Acompanhamento de Distribui√ß√£o em Salas\"],\n",
    "                \"SAVI\": [\"Sistema de Acompanhamento de Video Dom√©stico\"],\n",
    "                \"SRPTV\": [\"Sistema de Recep√ß√£o de Programa√ß√£o de TV Paga\"],\n",
    "                \"SAD\": [\"Sistema Ancine Digital\"],\n",
    "                \"CPB\": [\"Certificado de Produto Brasileiro\"],\n",
    "                \"ROE\": [\"Registro de Obra Estrangeira\"],\n",
    "                \"CRT\": [\"Certificado de Registro de T√≠tulo\"]\n",
    "            }\n",
    "            \n",
    "            expanded_query = query\n",
    "            for key, alternatives in expansions.items():\n",
    "                if key.lower() in query.lower():\n",
    "                    for alt in alternatives:\n",
    "                        if alt.lower() not in query.lower():\n",
    "                            expanded_query += f\" {alt}\"\n",
    "            \n",
    "            return expanded_query\n",
    "        \n",
    "        # Criamos um wrapper para a chain original que processar√° os documentos\n",
    "        def chain_with_preprocessing(inputs):\n",
    "            # Processar a query para melhorar a recupera√ß√£o\n",
    "            question = inputs[\"question\"]\n",
    "            expanded_question = process_query(question)\n",
    "            \n",
    "            # NOVO C√ìDIGO ADICIONADO AQUI\n",
    "            # Recuperar documentos relevantes\n",
    "            docs = retriever.get_relevant_documents(expanded_question)\n",
    "            \n",
    "            # Fazer an√°lise preliminar das INs e artigos\n",
    "            ins_mentioned = {}\n",
    "            for doc in docs:\n",
    "                if 'instrucao_normativa' in doc.metadata:\n",
    "                    in_num = doc.metadata['instrucao_normativa']\n",
    "                    if in_num not in ins_mentioned:\n",
    "                        ins_mentioned[in_num] = []\n",
    "                        \n",
    "                    # Extrair artigos mencionados no conte√∫do\n",
    "                    art_pattern = r'Art[\\.\\s]*\\s*(\\d+)'\n",
    "                    art_matches = re.findall(art_pattern, doc.page_content, re.IGNORECASE)\n",
    "                    ins_mentioned[in_num].extend([f\"Art. {m}\" for m in art_matches])\n",
    "            \n",
    "            # Criar resumo contextual\n",
    "            context_summary = \"\\n\\nINFORMA√á√ïES DE CONTEXTO IMPORTANTES:\\n\"\n",
    "            for in_num, articles in ins_mentioned.items():\n",
    "                unique_articles = list(set(articles))\n",
    "                context_summary += f\"- {in_num} cont√©m os seguintes artigos relevantes: {', '.join(unique_articles[:10])}\\n\"\n",
    "            \n",
    "            # Combinar com a query expandida\n",
    "            final_question = f\"{expanded_question}\\n\\n{context_summary}\"\n",
    "            # FIM DO NOVO C√ìDIGO\n",
    "            \n",
    "            # Executar a chain com a query final aprimorada\n",
    "            result = chain({\"question\": final_question})\n",
    "            \n",
    "            # Processar os documentos de origem antes de retornar\n",
    "            if \"source_documents\" in result:\n",
    "                result[\"source_documents\"] = enrich_documents(result[\"source_documents\"])\n",
    "                \n",
    "            return result\n",
    "        \n",
    "        # Retornamos a fun√ß√£o wrapped em vez da chain direta\n",
    "        return chain_with_preprocessing\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Erro ao criar a chain: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_chat_history():\n",
    "    \"\"\"Exibe o hist√≥rico de conversa na interface\"\"\"\n",
    "    for message in st.session_state.messages:\n",
    "        if message[\"role\"] == \"human\":\n",
    "            with st.chat_message(\"user\"):\n",
    "                st.write(message[\"content\"])\n",
    "        else:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(message[\"content\"])\n",
    "\n",
    "\n",
    "def extract_reference_summary(docs):\n",
    "    \"\"\"Extrai um resumo das refer√™ncias encontradas para mostrar ao usu√°rio\"\"\"\n",
    "    if not docs:\n",
    "        return \"Nenhuma fonte espec√≠fica encontrada.\"\n",
    "    \n",
    "    references = []\n",
    "    ins_mentioned = set()\n",
    "    articles_mentioned = set()\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Extrai IN\n",
    "        if 'instrucao_normativa' in doc.metadata:\n",
    "            ins_mentioned.add(doc.metadata['instrucao_normativa'])\n",
    "        \n",
    "        # Extrai artigos mencionados\n",
    "        if 'artigos' in doc.metadata and doc.metadata['artigos']:\n",
    "            for art in doc.metadata['artigos']:\n",
    "                articles_mentioned.add(art)\n",
    "        \n",
    "        # Extrai fonte do documento\n",
    "        if 'source' in doc.metadata:\n",
    "            filename = os.path.basename(doc.metadata['source'])\n",
    "            references.append(filename)\n",
    "    \n",
    "    # Formata resumo\n",
    "    summary = []\n",
    "    if articles_mentioned:\n",
    "        summary.append(f\"**Artigos mencionados:** {', '.join(sorted(articles_mentioned))}\")\n",
    "    \n",
    "    if ins_mentioned:\n",
    "        summary.append(f\"**Instru√ß√µes Normativas:** {', '.join(sorted(ins_mentioned))}\")\n",
    "    \n",
    "    unique_refs = list(set(references))\n",
    "    if unique_refs:\n",
    "        summary.append(f\"**Documentos consultados:** {', '.join(unique_refs[:3])}\")\n",
    "    \n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configura√ß√£o da p√°gina\n",
    "    st.set_page_config(\n",
    "        page_title=\"Chatbot da ANCINE\",\n",
    "        page_icon=\"üé¨\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    # Inicializa vari√°veis de estado\n",
    "    init_session_state()\n",
    "    \n",
    "    # Interface principal\n",
    "    st.title(\"üé¨ Chatbot de Instru√ß√µes Normativas da ANCINE\")\n",
    "    st.markdown(\"\"\"\n",
    "    Tire suas d√∫vidas sobre instru√ß√µes normativas e regulamenta√ß√µes do segmento de regula√ß√£o e registro do mercado audiovisual brasileiro .\n",
    "    Este chatbot consulta a base de documentos oficiais da ANCINE para fornecer informa√ß√µes precisas.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Sidebar para configura√ß√µes\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configura√ß√µes\")\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            if st.button(\"üîÑ Reiniciar Chat\", use_container_width=True):\n",
    "                st.session_state.chain = None\n",
    "                st.success(\"Chatbot reiniciado.\")\n",
    "        \n",
    "        with col2:\n",
    "            if st.button(\"üóëÔ∏è Limpar Hist√≥rico\", use_container_width=True):\n",
    "                st.session_state.messages = []\n",
    "                st.session_state.memory = ConversationBufferMemory(\n",
    "                    memory_key=\"chat_history\",\n",
    "                    return_messages=True,\n",
    "                    output_key=\"answer\"\n",
    "                )\n",
    "                st.session_state.chain = None\n",
    "                st.rerun()  # Substitu√≠do experimental_rerun por rerun\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"\"\"\n",
    "        ### Exemplos de perguntas:\n",
    "        - O que √© uma obra brasileira independente?\n",
    "        - Quais os requisitos para cadastro de obra?\n",
    "        - Como fa√ßo para registrar um projeto na ANCINE?\n",
    "        - O que diz a IN 95 sobre produ√ß√£o?\n",
    "        - Classifica√ß√£o indicativa segundo a ANCINE\n",
    "        \"\"\")\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"Desenvolvido com ‚ù§Ô∏è usando LangChain e Streamlit para a ENAP\")\n",
    "    \n",
    "    # Inicializa a chain se ainda n√£o existir\n",
    "    if st.session_state.chain is None:\n",
    "        with st.spinner(\"Inicializando o chatbot especialista em ANCINE...\"):\n",
    "            st.session_state.chain = create_chain()\n",
    "            if st.session_state.chain is None:\n",
    "                st.error(\"N√£o foi poss√≠vel inicializar o chatbot. Verifique sua API Key no arquivo .env\")\n",
    "                return\n",
    "    \n",
    "    # Exibe hist√≥rico de mensagens\n",
    "    display_chat_history()\n",
    "    \n",
    "    # Campo de entrada do usu√°rio\n",
    "    user_query = st.chat_input(\"Digite sua pergunta sobre instru√ß√µes normativas da ANCINE...\")\n",
    "    \n",
    "    # Processamento da pergunta\n",
    "    if user_query:\n",
    "        # Adiciona a pergunta ao hist√≥rico\n",
    "        st.session_state.messages.append({\"role\": \"human\", \"content\": user_query})\n",
    "        \n",
    "        # Exibe a pergunta\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(user_query)\n",
    "        \n",
    "        # Exibe indicador de \"pensando\"\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            message_placeholder.markdown(\"üîç Consultando a base de INs da ANCINE...\")\n",
    "            \n",
    "            try:\n",
    "                # Executa a chain para obter a resposta\n",
    "                start_time = time.time()\n",
    "                response = st.session_state.chain({\"question\": user_query})\n",
    "                end_time = time.time()\n",
    "                \n",
    "                # Formata a resposta\n",
    "                answer = response.get(\"answer\", \"Desculpe, n√£o consegui encontrar informa√ß√µes sobre este tema nas INs consultadas.\")\n",
    "                \n",
    "                # An√°lise e resumo das fontes\n",
    "                source_docs = response.get(\"source_documents\", [])\n",
    "                if source_docs:\n",
    "                    reference_summary = extract_reference_summary(source_docs)\n",
    "                    answer += f\"\\n\\n---\\n{reference_summary}\"\n",
    "                \n",
    "                # Adiciona tempo de resposta\n",
    "                answer += f\"\\n\\n*Tempo de resposta: {end_time - start_time:.2f} segundos*\"\n",
    "                \n",
    "                # Atualiza o placeholder com a resposta\n",
    "                message_placeholder.markdown(answer)\n",
    "                \n",
    "                # Salva a resposta no hist√≥rico\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Erro ao processar sua pergunta: {str(e)}\"\n",
    "                message_placeholder.error(error_msg)\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
