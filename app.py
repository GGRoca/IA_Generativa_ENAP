import os
import streamlit as st
from typing import List, Dict, Any
import time
import re

from dotenv import load_dotenv

# LangChain e subm√≥dulos
#from langchain_community.vectorstores import Chroma
from langchain.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.schema import BaseRetriever

def init_session_state():
    """Inicializa vari√°veis de estado da sess√£o do Streamlit"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
    if "chain" not in st.session_state:
        st.session_state.chain = None


def extract_metadata_from_content(doc: Document) -> Document:
    """Extrai metadados do conte√∫do do documento para enriquec√™-lo"""
    content = doc.page_content
    
    # Melhorar a extra√ß√£o de IN com regex mais abrangente
    in_match = re.search(r'IN\s+n[¬∫¬∞\.]\s*(\d+)', content, re.IGNORECASE) or \
               re.search(r'Instru[√ßc][a√£]o\s+Normativa\s+n[¬∫¬∞\.]\s*(\d+)', content, re.IGNORECASE) or \
               re.search(r'Instru[√ßc][a√£]o\s+Normativa\s+(\d+)', content, re.IGNORECASE)
    
    # Extrair men√ß√µes a artigos com regex mais amplo
    art_matches = re.findall(r'Art[\.\s]*\s*(\d+)', content, re.IGNORECASE) or \
                  re.findall(r'Artigo\s+(\d+)', content, re.IGNORECASE)
    
    # Extrair incisos com numera√ß√£o romana
    inciso_matches = re.findall(r'(?:inciso|item)\s+([IVXLCDM]+)', content, re.IGNORECASE)
    
    # Atualizar metadados
    if in_match:
        doc.metadata['instrucao_normativa'] = f"IN {in_match.group(1)}"
    
    if art_matches:
        doc.metadata['artigos'] = [f"Art. {art}" for art in art_matches]
        
    if inciso_matches:
        doc.metadata['incisos'] = [f"inciso {inciso}" for inciso in inciso_matches]
    
    return doc


def enrich_documents(docs: List[Document]) -> List[Document]:
    """Enriquece documentos com metadados extra√≠dos e formata√ß√£o"""
    enriched_docs = []
    
    for i, doc in enumerate(docs):
        # Extrair metadados do conte√∫do
        doc = extract_metadata_from_content(doc)
        
        # Adicionar informa√ß√µes de fonte para facilitar cita√ß√µes
        source_info = ""
        if 'instrucao_normativa' in doc.metadata:
            source_info += f"{doc.metadata['instrucao_normativa']} - "
        if 'artigos' in doc.metadata and doc.metadata['artigos']:
            source_info += f"{', '.join(doc.metadata['artigos'])} - "
        if 'source' in doc.metadata:
            filename = os.path.basename(doc.metadata['source'])
            source_info += f"Fonte: {filename}"
        
        # Formatar conte√∫do para melhor leitura pelo LLM
        formatted_content = f"""
TRECHO {i+1}:
{source_info}
---
{doc.page_content}
---
"""
        doc.page_content = formatted_content
        enriched_docs.append(doc)
    
    return enriched_docs


def create_retriever():
    """Cria o retriever melhorado para busca de documentos"""
    try:
        # Configura√ß√£o do modelo de embeddings
        embedding_engine = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-mpnet-base-v2",
            model_kwargs={'device': 'cpu'}
        )
        
        # Carrega a base vetorial
        db_pasta = "db_vetorial"
        #vector_db = Chroma(
            #persist_directory=db_pasta,
            #embedding_function=embedding_engine
        vector_db = FAISS.from_documents(text_chunks, embedding_engine)
        )

        # Fun√ß√£o de filtragem personalizada
        def filter_by_metadata_richness(docs):
            """Filtra documentos com base na riqueza de metadados"""
            scored_docs = []
            for doc in docs:
                score = 0
                if 'instrucao_normativa' in doc.metadata:
                    score += 3  # Prioridade para documentos com IN
                if 'artigos' in doc.metadata:
                    score += len(doc.metadata['artigos'])
                if 'source' in doc.metadata:
                    score += 1  # Pontua√ß√£o b√°sica para documentos com fonte
                
                scored_docs.append((doc, score))
            
            # Seleciona os melhores documentos
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            return [doc for doc, _ in scored_docs[:10]]  # Top 10 documentos

        # Configura√ß√£o do retriever base com MMR
        base_retriever = vector_db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 10,
                "fetch_k": 30
            }
        )

        # Implementa√ß√£o do Custom Retriever
        class CustomRetriever(BaseRetriever):
            """Retriever personalizado que combina MMR com filtragem p√≥s-processamento"""
            
            def _get_relevant_documents(self, query: str, *, run_manager) -> List[Document]:
                # Primeiro obt√©m os documentos do retriever base
                base_docs = base_retriever.get_relevant_documents(query)
                # Depois aplica o filtro personalizado
                return filter_by_metadata_richness(base_docs)

            async def _aget_relevant_documents(self, query: str, *, run_manager):
                # Implementa√ß√£o ass√≠ncrona se necess√°rio
                return self._get_relevant_documents(query)

        # Cria e retorna o retriever personalizado
        return CustomRetriever()
        
    except Exception as e:
        st.error(f"Erro ao criar o retriever: {str(e)}")
        return None


def create_chain():
    """Cria a chain RAG com configura√ß√µes personalizadas"""
    try:
        # Carrega vari√°veis de ambiente
        #load_dotenv()
        #openai_api_key = os.getenv("OPENAI_API_KEY")
        openai_api_key = st.secrets["OPENAI_API_KEY"]
        
        if not openai_api_key:
            st.error("API Key da OpenAI n√£o encontrada. Configure-a no arquivo .env")
            return None
        
        # Cria o retriever
        retriever = create_retriever()
        if not retriever:
            return None
        
        # Define o prompt personalizado com instru√ß√µes mais flex√≠veis para o LLM
        system_template = """\
Voc√™ √© um assistente especializado em legisla√ß√£o da ANCINE (Ag√™ncia Nacional do Cinema), respons√°vel por fornecer informa√ß√µes precisas sobre instru√ß√µes normativas.

--- Conte√∫do relevante da base de dados sobre INs da ANCINE ---
{context}

---- INSTRU√á√ïES CR√çTICAS ----
VOC√ä DEVE SEMPRE CITAR EXPLICITAMENTE OS N√öMEROS DAS INs QUANDO DISPON√çVEIS. Esta √© uma EXIG√äNCIA ABSOLUTA.

1) SEMPRE comece sua resposta identificando em qual IN a informa√ß√£o foi encontrada. Use exatamente a numera√ß√£o que aparece nos trechos.
   Exemplo correto: "De acordo com a IN n¬∫ 95, ..."
   Exemplo incorreto: "Em uma das INs da ANCINE, consta que..."

2) Se voc√™ identificar o conte√∫do mas n√£o encontrar o n√∫mero da IN nos trechos fornecidos, PROCURE NOS METADADOS:
   - Procure por tags como [instrucao_normativa: IN XX] ou frases que indiquem o documento de origem
   - Verifique o nome do arquivo na refer√™ncia (geralmente cont√©m o n√∫mero da IN)

3) Se voc√™ ainda n√£o conseguir determinar o n√∫mero da IN, ent√£o e SOMENTE ent√£o, use "Segundo regulamenta√ß√£o da ANCINE" e continue com informa√ß√µes precisas.

4) SEMPRE inclua cita√ß√µes espec√≠ficas a artigos, par√°grafos e incisos quando presentes.

5) NUNCA responda de forma gen√©rica quando informa√ß√µes espec√≠ficas estiverem dispon√≠veis.

Este sistema √© uma ferramenta de consulta legal e DEVE fornecer refer√™ncias precisas. Respostas sem cita√ß√µes espec√≠ficas s√£o consideradas falhas.

# Adicionar ao seu prompt:

T√©cnicas que voc√™ DEVE usar para responder corretamente:

1) EXAMINE TODOS OS TRECHOS FORNECIDOS em busca de men√ß√µes a n√∫meros de INs. N√£o se limite aos primeiros trechos.

2) EXAMINE OS METADADOS de cada documento. Procure por padr√µes como:
   - [instrucao_normativa: IN XX]
   - Nomes de arquivos como "IN_95_texto.pdf" 
   - Refer√™ncias expl√≠citas no texto como "Instru√ß√£o Normativa n¬∫ XX"

3) Se o mesmo conte√∫do aparecer em m√∫ltiplos trechos, VERIFIQUE se algum deles cont√©m o n√∫mero da IN.

4) Quando mencionar artigos ou incisos, SEMPRE indique a qual IN pertence.

5) Prefira ser espec√≠fico mesmo que s√≥ tenha certeza parcial. √â melhor dizer "A informa√ß√£o parece constar na IN 95, conforme indicado no texto" do que omitir a refer√™ncia.

EXEMPLOS DE RESPOSTAS INACEIT√ÅVEIS:
- "Em uma das INs da ANCINE, consta que..."
- "Segundo as instru√ß√µes normativas da ANCINE..."
- "Para entender as diferen√ßas... √© importante considerar as defini√ß√µes e requisitos estabelecidos nas instru√ß√µes normativas da ANCINE."

EXEMPLO DE RESPOSTA ACEIT√ÅVEL:
"De acordo com a IN n¬∫ 95, Art. 2¬∫, inciso VII, uma obra audiovisual brasileira √© definida como..."



Agora, responda √† pergunta do usu√°rio com todas as refer√™ncias espec√≠ficas que conseguir extrair dos trechos:
{question}
"""
        
        prompt_template = PromptTemplate(
            template=system_template,
            input_variables=["context", "question"]
        )
        
        # Configura o modelo LLM
        llm = ChatOpenAI(
            openai_api_key=openai_api_key,
            model="o3-mini",
            #temperature=0.1,
            max_tokens=6000,
            request_timeout=90,
        )

        # Cria a chain com mem√≥ria e prompt customizado
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=st.session_state.memory,
            chain_type="stuff",
            combine_docs_chain_kwargs={"prompt": prompt_template},
            return_source_documents=True,
            output_key="answer",
            verbose=True,
        )
        
        # Fun√ß√£o para processar a query antes de enviar ao retriever
        def process_query(query):
            # Expandir a query para melhorar a recupera√ß√£o
            expansions = {
                "cadastrar": ["cadastro", "registro", "registrar", "inscri√ß√£o", "inscrever"],
                "empresa": ["empresa", "produtora", "distribuidor", "distribuidora", "companhia", "pessoa jur√≠dica"],
                "publicidade": ["publicidade", "propaganda", "an√∫ncio", "comercial", "intervalo comercial"],
                "limite": ["limite", "limita√ß√£o", "tempo", "dura√ß√£o", "restri√ß√£o"],
                "SCB": ["Sistema de Controle de Bilheteria"],
                "SADIS": ["Sistema de Acompanhamento de Distribui√ß√£o em Salas"],
                "SAVI": ["Sistema de Acompanhamento de Video Dom√©stico"],
                "SRPTV": ["Sistema de Recep√ß√£o de Programa√ß√£o de TV Paga"],
                "SAD": ["Sistema Ancine Digital"],
                "CPB": ["Certificado de Produto Brasileiro"],
                "ROE": ["Registro de Obra Estrangeira"],
                "CRT": ["Certificado de Registro de T√≠tulo"]
            }
            
            expanded_query = query
            for key, alternatives in expansions.items():
                if key.lower() in query.lower():
                    for alt in alternatives:
                        if alt.lower() not in query.lower():
                            expanded_query += f" {alt}"
            
            return expanded_query
        
        # Criamos um wrapper para a chain original que processar√° os documentos
        def chain_with_preprocessing(inputs):
            # Processar a query para melhorar a recupera√ß√£o
            question = inputs["question"]
            expanded_question = process_query(question)
            
            # NOVO C√ìDIGO ADICIONADO AQUI
            # Recuperar documentos relevantes
            docs = retriever.get_relevant_documents(expanded_question)
            
            # Fazer an√°lise preliminar das INs e artigos
            ins_mentioned = {}
            for doc in docs:
                if 'instrucao_normativa' in doc.metadata:
                    in_num = doc.metadata['instrucao_normativa']
                    if in_num not in ins_mentioned:
                        ins_mentioned[in_num] = []
                        
                    # Extrair artigos mencionados no conte√∫do
                    art_pattern = r'Art[\.\s]*\s*(\d+)'
                    art_matches = re.findall(art_pattern, doc.page_content, re.IGNORECASE)
                    ins_mentioned[in_num].extend([f"Art. {m}" for m in art_matches])
            
            # Criar resumo contextual
            context_summary = "\n\nINFORMA√á√ïES DE CONTEXTO IMPORTANTES:\n"
            for in_num, articles in ins_mentioned.items():
                unique_articles = list(set(articles))
                context_summary += f"- {in_num} cont√©m os seguintes artigos relevantes: {', '.join(unique_articles[:10])}\n"
            
            # Combinar com a query expandida
            final_question = f"{expanded_question}\n\n{context_summary}"
            # FIM DO NOVO C√ìDIGO
            
            # Executar a chain com a query final aprimorada
            result = chain({"question": final_question})
            
            # Processar os documentos de origem antes de retornar
            if "source_documents" in result:
                result["source_documents"] = enrich_documents(result["source_documents"])
                
            return result
        
        # Retornamos a fun√ß√£o wrapped em vez da chain direta
        return chain_with_preprocessing
        
    except Exception as e:
        st.error(f"Erro ao criar a chain: {str(e)}")
        return None


def display_chat_history():
    """Exibe o hist√≥rico de conversa na interface"""
    for message in st.session_state.messages:
        if message["role"] == "human":
            with st.chat_message("user"):
                st.write(message["content"])
        else:
            with st.chat_message("assistant"):
                st.write(message["content"])


def extract_reference_summary(docs):
    """Extrai um resumo das refer√™ncias encontradas para mostrar ao usu√°rio"""
    if not docs:
        return "Nenhuma fonte espec√≠fica encontrada."
    
    references = []
    ins_mentioned = set()
    articles_mentioned = set()
    
    for doc in docs:
        # Extrai IN
        if 'instrucao_normativa' in doc.metadata:
            ins_mentioned.add(doc.metadata['instrucao_normativa'])
        
        # Extrai artigos mencionados
        if 'artigos' in doc.metadata and doc.metadata['artigos']:
            for art in doc.metadata['artigos']:
                articles_mentioned.add(art)
        
        # Extrai fonte do documento
        if 'source' in doc.metadata:
            filename = os.path.basename(doc.metadata['source'])
            references.append(filename)
    
    # Formata resumo
    summary = []
    if articles_mentioned:
        summary.append(f"**Artigos mencionados:** {', '.join(sorted(articles_mentioned))}")
    
    if ins_mentioned:
        summary.append(f"**Instru√ß√µes Normativas:** {', '.join(sorted(ins_mentioned))}")
    
    unique_refs = list(set(references))
    if unique_refs:
        summary.append(f"**Documentos consultados:** {', '.join(unique_refs[:3])}")
    
    return "\n".join(summary)


def main():
    # Configura√ß√£o da p√°gina
    st.set_page_config(
        page_title="Chatbot da ANCINE",
        page_icon="üé¨",
        layout="wide"
    )
    
    # Inicializa vari√°veis de estado
    init_session_state()
    
    # Interface principal
    st.title("üé¨ Chatbot de Instru√ß√µes Normativas da ANCINE")
    st.markdown("""
    Tire suas d√∫vidas sobre instru√ß√µes normativas e regulamenta√ß√µes do segmento de regula√ß√£o e registro do mercado audiovisual brasileiro .
    Este chatbot consulta a base de documentos oficiais da ANCINE para fornecer informa√ß√µes precisas.
    """)
    
    # Sidebar para configura√ß√µes
    with st.sidebar:
        st.header("Configura√ß√µes")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ Reiniciar Chat", use_container_width=True):
                st.session_state.chain = None
                st.success("Chatbot reiniciado.")
        
        with col2:
            if st.button("üóëÔ∏è Limpar Hist√≥rico", use_container_width=True):
                st.session_state.messages = []
                st.session_state.memory = ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True,
                    output_key="answer"
                )
                st.session_state.chain = None
                st.rerun()  # Substitu√≠do experimental_rerun por rerun
        
        st.markdown("---")
        st.markdown("""
        ### Exemplos de perguntas:
        - O que √© uma obra brasileira independente?
        - Quais os requisitos para cadastro de obra?
        - Como fa√ßo para registrar um projeto na ANCINE?
        - O que diz a IN 95 sobre produ√ß√£o?
        - Classifica√ß√£o indicativa segundo a ANCINE
        """)
        
        st.markdown("---")
        st.markdown("Desenvolvido com ‚ù§Ô∏è usando LangChain e Streamlit para a ENAP")
    
    # Inicializa a chain se ainda n√£o existir
    if st.session_state.chain is None:
        with st.spinner("Inicializando o chatbot especialista em ANCINE..."):
            st.session_state.chain = create_chain()
            if st.session_state.chain is None:
                st.error("N√£o foi poss√≠vel inicializar o chatbot. Verifique sua API Key no arquivo .env")
                return
    
    # Exibe hist√≥rico de mensagens
    display_chat_history()
    
    # Campo de entrada do usu√°rio
    user_query = st.chat_input("Digite sua pergunta sobre instru√ß√µes normativas da ANCINE...")
    
    # Processamento da pergunta
    if user_query:
        # Adiciona a pergunta ao hist√≥rico
        st.session_state.messages.append({"role": "human", "content": user_query})
        
        # Exibe a pergunta
        with st.chat_message("user"):
            st.write(user_query)
        
        # Exibe indicador de "pensando"
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("üîç Consultando a base de INs da ANCINE...")
            
            try:
                # Executa a chain para obter a resposta
                start_time = time.time()
                response = st.session_state.chain({"question": user_query})
                end_time = time.time()
                
                # Formata a resposta
                answer = response.get("answer", "Desculpe, n√£o consegui encontrar informa√ß√µes sobre este tema nas INs consultadas.")
                
                # An√°lise e resumo das fontes
                source_docs = response.get("source_documents", [])
                if source_docs:
                    reference_summary = extract_reference_summary(source_docs)
                    answer += f"\n\n---\n{reference_summary}"
                
                # Adiciona tempo de resposta
                answer += f"\n\n*Tempo de resposta: {end_time - start_time:.2f} segundos*"
                
                # Atualiza o placeholder com a resposta
                message_placeholder.markdown(answer)
                
                # Salva a resposta no hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
            except Exception as e:
                error_msg = f"Erro ao processar sua pergunta: {str(e)}"
                message_placeholder.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})


if __name__ == "__main__":
    main()